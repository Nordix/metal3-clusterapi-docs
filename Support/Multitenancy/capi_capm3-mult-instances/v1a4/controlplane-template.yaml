apiVersion: controlplane.cluster.x-k8s.io/v1alpha4
kind: KubeadmControlPlane
metadata:
  name: test1
  namespace: test2
spec:
  kubeadmConfigSpec:
    clusterConfiguration: {}
    files:
    - content: |
        #!/bin/bash
        set -e
        url="$1"
        dst="$2"
        filename="$(basename $url)"
        tmpfile="/tmp/$filename"
        curl -sSL -w "%{http_code}" "$url" | sed "s:/usr/bin:/usr/local/bin:g" > /tmp/"$filename"
        http_status=$(cat "$tmpfile" | tail -n 1)
        if [ "$http_status" != "200" ]; then
          echo "Error: unable to retrieve $filename file";
          exit 1;
        else
          cat "$tmpfile"| sed '$d' > "$dst";
        fi
      owner: root:root
      path: /usr/local/bin/retrieve.configuration.files.sh
      permissions: "0755"
    - content: |
        #!/bin/bash
        while :; do
          curl -sk https://127.0.0.1:6443/healthz 1>&2 > /dev/null
          isOk=$?
          isActive=$(systemctl show -p ActiveState keepalived.service | cut -d'=' -f2)
          if [ $isOk == "0" ] &&  [ $isActive != "active" ]; then
            logger 'API server is healthy, however keepalived is not running, starting keepalived'
            echo 'API server is healthy, however keepalived is not running, starting keepalived'
            sudo systemctl start keepalived.service
          elif [ $isOk != "0" ] &&  [ $isActive == "active" ]; then
            logger 'API server is not healthy, however keepalived running, stopping keepalived'
            echo 'API server is not healthy, however keepalived running, stopping keepalived'
            sudo systemctl stop keepalived.service
          fi
          sleep 5
        done
      owner: root:root
      path: /usr/local/bin/monitor.keepalived.sh
      permissions: "0755"
    - content: |
        [Unit]
        Description=Monitors keepalived adjusts status with that of API server
        After=syslog.target network-online.target
        [Service]
        Type=simple
        Restart=always
        ExecStart=/usr/local/bin/monitor.keepalived.sh
        [Install]
        WantedBy=multi-user.target
      owner: root:root
      path: /lib/systemd/system/monitor.keepalived.service
    - content: |
        ! Configuration File for keepalived
        global_defs {
            notification_email {
            sysadmin@example.com
            support@example.com
            }
            notification_email_from lb@example.com
            smtp_server localhost
            smtp_connect_timeout 30
        }
        vrrp_instance VI_1 {
            state MASTER
            interface eth1
            virtual_router_id 1
            priority 101
            advert_int 1
            virtual_ipaddress {
                192.168.111.250
            }
        }
      path: /etc/keepalived/keepalived.conf
    - content: |
        [connection]
        id=eth0
        type=ethernet
        interface-name=eth0
        master=ironicendpoint
        slave-type=bridge
        autoconnect=yes
        autoconnect-priority=999
      owner: root:root
      path: /etc/NetworkManager/system-connections/eth0.nmconnection
      permissions: "0600"
    - content: |
        [connection]
        id=ironicendpoint
        type=bridge
        interface-name=ironicendpoint

        [bridge]
        stp=false

        [ipv4]
        address1={{ ds.meta_data.provisioningIP }}/{{ ds.meta_data.provisioningCIDR }}
        method=manual

        [ipv6]
        addr-gen-mode=eui64
        method=ignore
      owner: root:root
      path: /etc/NetworkManager/system-connections/ironicendpoint.nmconnection
      permissions: "0600"
    - content: |
        [kubernetes]
        name=Kubernetes
        baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
        enabled=1
        gpgcheck=1
        repo_gpgcheck=0
        gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
      owner: root:root
      path: /etc/yum.repos.d/kubernetes.repo
      permissions: "0644"
    - content: |
        [registries.search]
        registries = ['docker.io']

        [registries.insecure]
        registries = ['192.168.111.2:5000']
      path: /etc/containers/registries.conf
    initConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          cgroup-driver: systemd
          container-runtime: remote
          container-runtime-endpoint: unix:///var/run/crio/crio.sock
          feature-gates: AllAlpha=false
          node-labels: metal3.io/uuid={{ ds.meta_data.uuid }}
          provider-id: metal3://{{ ds.meta_data.uuid }}
          runtime-request-timeout: 5m
        name: '{{ ds.meta_data.name }}'
    joinConfiguration:
      controlPlane: {}
      nodeRegistration:
        kubeletExtraArgs:
          cgroup-driver: systemd
          container-runtime: remote
          container-runtime-endpoint: unix:///var/run/crio/crio.sock
          feature-gates: AllAlpha=false
          node-labels: metal3.io/uuid={{ ds.meta_data.uuid }}
          provider-id: metal3://{{ ds.meta_data.uuid }}
          runtime-request-timeout: 5m
        name: '{{ ds.meta_data.name }}'
    postKubeadmCommands:
    - mkdir -p /home/metal3/.kube
    - chown metal3:metal3 /home/metal3/.kube
    - cp /etc/kubernetes/admin.conf /home/metal3/.kube/config
    - chown metal3:metal3 /home/metal3/.kube/config
    preKubeadmCommands:
    - systemctl restart NetworkManager.service
    - nmcli connection load /etc/NetworkManager/system-connections/eth0.nmconnection
    - nmcli connection up eth0
    - nmcli connection load /etc/NetworkManager/system-connections/ironicendpoint.nmconnection
    - nmcli connection up ironicendpoint
    - systemctl enable --now crio keepalived kubelet
    - systemctl enable --now /lib/systemd/system/monitor.keepalived.service
    users:
    - name: metal3
      sshAuthorizedKeys:
      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCpobtItGp5RgVT5zZoTg8mnjxq5mvHlI9KnVAtHFF1U9OhxFVUswq/XJf55yguJeZq0iZPD1ejT1VFa8OiaKVZxDfjspa4vzWXshh77/HMvlJ/uHQKhLdmibZ7+8vU0GMKhmH1kYD/A0U36oIyUwtl9F7R2eNEsMSbKonNw50ZA/YmSqJRahZVXDuTR6u31GynHWnUEFtj0PKGapL4pB5sNfxc5gkca9EDnUobDLfFpqO1mlyG0rR+NzxbnAoCOFjiVoVYZuhOyFeti+QDZwo6m2G4sCy0b8n0dkt816OdZdQH8NcH0lDfuZuN5zls121w8ULmCA5us3VJyQptOXlOXOxrSuUIsp37UG0WlvDcUN2fmWFUG2ALtrKz9yY3D3nGTNugl/LydXGTXTHbz+lC/vnNLdetJ5cPEYqqaohXX1qJd44iWxX3w5q5kE4mmPHTmP6sDkSTaYhYkFvsmT9fDEqWELFF11x6aM4pA48bKTmeGKbIZAd+3eJht2yMD+M= cloud-user@mohammed-v1a4.openstacklocal
      sudo: ALL=(ALL) NOPASSWD:ALL
  machineTemplate:
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1alpha5
      kind: Metal3MachineTemplate
      name: test1-controlplane
    nodeDrainTimeout: 0s
  replicas: 1
  rolloutStrategy:
    rollingUpdate:
      maxSurge: 1
  version: v1.25.2
---
apiVersion: infrastructure.cluster.x-k8s.io/v1alpha5
kind: Metal3MachineTemplate
metadata:
  name: test1-controlplane
  namespace: test2
spec:
  template:
    spec:
      dataTemplate:
        name: test1-controlplane-template
      image:
        checksum: http://172.23.0.1/images/CENTOS_9_NODE_IMAGE_K8S_v1.25.2-raw.img.md5sum
        checksumType: md5
        format: raw
        url: http://172.23.0.1/images/CENTOS_9_NODE_IMAGE_K8S_v1.25.2-raw.img
---
apiVersion: infrastructure.cluster.x-k8s.io/v1alpha5
kind: Metal3DataTemplate
metadata:
  name: test1-controlplane-template
  namespace: test2
spec:
  clusterName: test1
  metaData:
    ipAddressesFromIPPool:
    - key: provisioningIP
      name: provisioning-pool
    objectNames:
    - key: name
      object: machine
    - key: local-hostname
      object: machine
    - key: local_hostname
      object: machine
    prefixesFromIPPool:
    - key: provisioningCIDR
      name: provisioning-pool
  networkData:
    links:
      ethernets:
      - id: enp1s0
        macAddress:
          fromHostInterface: enp1s0
        type: phy
      - id: enp2s0
        macAddress:
          fromHostInterface: enp2s0
        type: phy
    networks:
      ipv4:
      - id: baremetalv4
        ipAddressFromIPPool: baremetalv4-pool
        link: enp2s0
        routes:
        - gateway:
            fromIPPool: baremetalv4-pool
          network: 0.0.0.0
          prefix: 0
    services:
      dns:
      - 8.8.8.8
